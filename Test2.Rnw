\documentclass[10pt, a4paper]{article}
\usepackage[a4paper, top = 2.5 cm, bottom = 2.5 cm, right= 2.5cm, left= 2.5cm]{geometry}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{footnote}
\usepackage{url}


\begin{document}

<<eval=T,echo=F>>=
library(knitr)

# Data and Setup:
rm(list=ls())
cat("\014")
options(scipen = 1, digits = 3)
@

\input{Titlepage.Rnw}

%------------------------- Question 2.1 -------------------------%
\section{Question 2.1}
We define the process \{$X_t$\} given by:
\begin{equation} \label{process}
X_t - 0.9X_{t-1} = \epsilon_t + 2\epsilon_{t-1} + 0.5\epsilon_{t-2}
\end{equation}
where $\epsilon_t$ is a white-noise process with $V[\epsilon_t]$ = $\sigma^2_{\epsilon}$ = $0.2^2$. For ease of notation, we will write \eqref{process} as: 
\begin{align} \label{process_better}
(1 - 0.9B)X_{t} &= (1 + 2B + 0.5B^2) \epsilon_t
\end{align} 
so that $\phi(B)X_t = \theta(B)\epsilon_t$, with $\phi_0 = 1, \phi_1 = -0.9, \theta_0 = 1, \theta_1 = 2$ and $\theta_2 = 0.5$. The operator B performs a backwards shift on the argument: $B^j X_t = X_{t-j}$. The process \{$X_t$\} is an ARMA(1,2) process.


%----------- ARMA -----------%
\subsection{The ARMA process}
We use ARMA processes to describe dynamic time series that are a function of their own preceding behaviour and a sequence of perturbations. For example, interpret \eqref{process} as this month's retail sales: they depend on what you could already sell last month, and on any advertising you made in two months time. 

Formally, according to [TSA](p 125, eq 5.95), an ARMA(p,q) process is comprised of two parts\footnote{MathWorks\textsuperscript{\textregistered}, \textit{Autoregressive Moving Average Model}. \url{https://nl.mathworks.com/help/econ/arma-model.html}}: the Auto-Regressive part describes how the current observation $X_t$ depends on \textit{p} past observations $X_{t-p}$, the Moving-Average part describes how $X_t$ depends on \textit{q} past innovations $\epsilon_{t-q}$. In this paper, $\epsilon_t$ will be regarded as white noise, independant identically distributed and sampled from $\epsilon_t \sim \mathcal{N}(0,\sigma^2_{\epsilon})$. 


%----------- Stationary -----------%
\subsection{Stationary and invertible} \label{stationary}
Let us study the ARMA process first through two powerful conditions. According to [TSA](p 99, def 5.3), a time series is  \textit{stationary} when its probability distribution function does not change over time. Consequently, statistical properties as the mean, average, autocorrelation, etc. do not change either. However, we will be working mostly with series that are \textit{weakly} stationary of order k, so that their first k'th order moments are invariant with respect to time. We will come back on this in Section \ref{second order}.
For some applications, we would like to write \eqref{process_better} in an MA-form, where all parameters are moved to the right-hand side of the equality sign. This requires the process to be \textit{invertible}. \\

We can determine whether \eqref{process_better} is stationary and/or invertible through the use of Theorem 5.12 in [TSA](p 125). A z-transformation gives us the transfer function:

\begin{equation}
H(z^{-1}) = \frac{\theta(z^{-1})}{\phi(z^{-1})} = \frac{1 + 2z^{-1} + 0.5z^{-2}}{1-0.9z^{-1}} 
= \frac{z^2 + 2z + 0.5}{z(z-0.9)}
\end{equation}
For the denominator, we have that $z_{1,2}$ = \{0, 0.9\}. All roots lie within the unit circle, so the process is stationary. In Section \ref{second order} we will however see that the process is weakly stationary. For the numerator, we have $z_{1,2} = -1 \pm 1/\sqrt{2}$. Not all roots lie within the unit circle, so the process is not invertible. 


%----------- 1st order -----------%
\subsection{First order moment representation}
Let us now focus on the expected value, the central first order moment:
\begin{align} 
X_t  &= 0.9X_{t-1} + \epsilon_t + 2\epsilon_{t-1} + 0.5\epsilon_{t-2} \\
E[X_t] &= 0.9 E[X_{t-1}] + E[\epsilon_t] + 2 E[\epsilon_{t-1}] + 0.5 E[\epsilon_{t-2}] = 0.9 E[X_{t-1}] \\
E[X_{t+1}] &= 0.9 E[X_{t}] + E[\epsilon_{t+1}] + 2 E[\epsilon_{t}] \: + \: 0.5 E[\epsilon_{t-1}] \:=\: 0.9 E[X_{t}] 
\end{align} 
where $E[\epsilon_t]$ is defined as zero, $E[\epsilon_{t+1}]$ = 0 as $\epsilon_{t+1}$ does not influence $X_t$, and $\lim_{k \to \infty} E[X_{t+k}] = 0$:
\begin{align}
E[X_{t+k}] &= 0.9^k E[X_{t}]
\end{align}
We expect an exponential decay of the expected value. 


%----------- 2nd order -----------%
\subsection{Second order moment representation} \label{second order}
\subsubsection{Autocovariance and autocorrelation functions}
We know the central second order moment as the variance: $\mu_2 (X) = V[X] = E[(X - E[X])^2]$. However, since V[$X_t$] = Cov[$X_t, X_t$] = $\gamma(0)$, the determination of the \textit{autocovariance} (the variance between two elements of the same series) will give us a more complete understanding of the second order moment representation. We will determine the autocovariance function as described in the handbook. We begin with the covariance of $\epsilon_t$ and $X_{t+k}$, according to [TSA](p 126, eq 5.97), where $\gamma_{\epsilon X}(k)$ = Cov[$\epsilon_t, X_{t+k}$]:
\begin{align}
  \begin{cases}
    (k = 0): \gamma_{\epsilon X}(0) - 0.9\gamma_{\epsilon X}(-1) &= \sigma^2_{\epsilon} = 0.04  \\
    (k = 1): \gamma_{\epsilon X}(1) - 0.9\gamma_{\epsilon X}(0) &= 2\sigma^2_{\epsilon} = 0.08  \\
    (k = 2): \gamma_{\epsilon X}(2) - 0.9\gamma_{\epsilon X}(1) &= 0.5\sigma^2_{\epsilon} = 0.02
  \end{cases}
\end{align}
\begin{align}
  \begin{cases}
    \gamma_{\epsilon X}(-1) &= 0  \\
    \gamma_{\epsilon X}(0) = \sigma^2_{\epsilon} &= 0.04  \\
    \gamma_{\epsilon X}(1) = (2 + 0.9)\sigma^2_{\epsilon} &= 0.116  \\
    \gamma_{\epsilon X}(2) = (0.5+0.9(2 + 0.9))\sigma^2_{\epsilon} &= 0.124 
  \end{cases}
\end{align}
For $\gamma(k)$ = Cov[$X_t, X_{t-k}$], the procedure is according to [TSA](p 126, eq 5.99):
\begin{align}
  \begin{cases}
    (k = 0): \gamma(0) + \phi_1 \gamma(1) &= \theta_0 \gamma_{\epsilon X}(0) + \theta_1 \gamma_{\epsilon X}(1) +
    \theta_2 \gamma_{\epsilon X}(2) \\
    (k = 1): \gamma(1) + \phi_1 \gamma(0) &= \theta_1 \gamma_{\epsilon X}(0) + \theta_2 \gamma_{\epsilon X}(1) \\
    (k = 2): \gamma(2) + \phi_1 \gamma(1) &= \theta_2 \gamma_{\epsilon X}(0) \\
    (k\geq3):\gamma(k) + \phi_1 \gamma(k-1) &= 0
  \end{cases}
\end{align}

\begin{align}
  \begin{cases}
    \gamma(0) - 0.9\gamma(1) &= 0.04 + 2\cdot 0.116 + 0.5\cdot 0.124 = 0.334 \\
    \gamma(1) - 0.9\gamma(0) &= 2\cdot 0.04 + 0.5\cdot 0.116 = 0.138 \\
    \gamma(2) - 0.9\gamma(1) &= 0.5\cdot 0.04 = 0.02 \\
    \gamma(k) \:\: (k\geq3) &= 0.9\gamma(k-1) 
  \end{cases}
\end{align}
Solving this linear system of equations, we have that:
<<eval=T,echo=F>>=
ml = 24    #maximum lag in all coming plots
coeff = matrix(c(1,-.9,0,0, -.9,1,0,0, 0,-.9,1,0, 0,0,-.9,1), 4, 4, byrow = T)
b = c(0.3342, 0.138, 0.02, 0)
gs = solve(coeff, b)
p = gs/gs[1]

for (i in 4:ml){
  gs = c(gs, 0.9*gs[i]) 
  p[i] = gs[i]/gs[1]
}
@

\begin{table}[h] \label{Table1}
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
    k           & 0  & 1  & 2  & 3  & 4  & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline
    $\gamma(k)$ & \Sexpr{gs[1]} & \Sexpr{gs[2]}  & \Sexpr{gs[3]} & \Sexpr{gs[4]}  & \Sexpr{gs[5]} & \Sexpr{gs[6]}  &          \Sexpr{gs[7]} & \Sexpr{gs[8]}  & \Sexpr{gs[9]}  & \Sexpr{gs[10]}  & \Sexpr{gs[11]}   \\ \hline
    $\rho(k)$   & \Sexpr{p[1]} & \Sexpr{p[2]} & \Sexpr{p[3]} & \Sexpr{p[4]} & \Sexpr{p[5]} & \Sexpr{p[6]}  & \Sexpr{p[7]}     & \Sexpr{p[8]}  & \Sexpr{p[9]}  & \Sexpr{p[10]}  &  \Sexpr{p[11]}  \\ \hline
  \end{tabular}
  \caption{Autocovariance and autocorrelation functions}
\end{table}

In Table 1, the \textit{autocorrelation function} $\rho(k)$ is given as well, giving an indication of the correlation between successive values of $X_t$. Here, $\rho(k)$ = $\gamma(k)/\gamma(1)$. In the coming text, we will refer to $\rho(k)$ as ACF(k). Since the the autocovariance function does depend on a time difference, the process is weakly stationary of order two.

\subsubsection{Partial autocorrelation functions}
The \textit{partial autocorrelation} is defined in [TSA](p 124, def 5.18) as the correlation between successive values of $X_t$, given all the observations in-between. We will determine the PACF $\phi_{k,k}$ through the use of Appendix B in [TSA](p357). Though since $\rho(0)$ is not defined as the element of a list, we will have to shift the expressions for $\rho(k)$ with one step. We become:

\begin{align}
\phi_{1,1} &= \rho(2) \\ 
\phi_{2,2} &= \frac{\rho(3) - \phi_{1,1}  \rho(2)}{1 - \phi_{1,1}  \rho(2)} = \frac{\rho(3) - \rho(2)^2}{1 - \rho(2)^2} \\
\phi_{2,1} &= \phi_{1,1} - \phi_{2,2} \phi_{1,1} \\
\phi_{3,3} &= \frac{\rho(4) - \phi_{2,1}  \rho(3) - \phi_{2,2}  \rho(2)}{1 - \phi_{2,1}  \rho(2) - \phi_{1,1}  \rho(3)} 
\end{align}
 
<<eval=T,echo=F>>=
phi = matrix(0, nrow = ml, ncol = ml)
phi[1,1] = p[2]

for (k in 1:(ml-1)){
  numerator = 0
  denominator = 0
  
  for (j in 1:k){
    numerator = numerator + phi[k,j]*p[k+2-j]
    denominator = denominator + phi[k,j]*p[j+1]
  }
  
  phi[k+1, k+1] = (p[k+2] - numerator)/(1 - denominator)

  for (j in 1:k){ phi[k+1, j] = phi[k, j] - phi[k+1, k+1]*phi[k, k+1-j] }
}
@

We can easily implement this behaviour in our code, since $\phi_{i,j}$ is a lower triangular matrix. First, we need to determine the diagonal element of row \textit{k+1} using all elements from the previous row: $\phi_{k,j}$ (j = 1, ... k). Then, all other elements in row \textit{k+1} are constructed using the diagonal element, so we need at least two loops. Through recursive calculations we obtain the main diagonal, as presented in Table 2:

\begin{table}[h] 
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
    k            & 1  & 2  & 3  & 4  & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline
    $\phi_{k,k}$   & \Sexpr{phi[1,1]} & \Sexpr{phi[2,2]} & \Sexpr{phi[3,3]} & \Sexpr{phi[4,4]} & \Sexpr{phi[5,5]} & \Sexpr{phi[6,6]}  & \Sexpr{phi[7,7]}     & \Sexpr{phi[8,8]}  & \Sexpr{phi[9,9]}  & \Sexpr{phi[10,10]}   \\ \hline
  \end{tabular}
  \caption{Partial autocorrelation function}
\end{table}

We can see that the decay of the PACF corresponds to an exponentially damped sine function.


%----------- Simulation -----------%
\subsection{Model Simulation}
\subsubsection{Plotting and discussion}
We will make 10 simulations of 200 observations of the ARMA(1,2) model. Now, according to the R manual for \texttt{arima}\footnote{R Documentation, \textit{ARIMA}. \url{https://stat.ethz.ch/R-manual/R-devel/library/stats/html/arima.html}}, the models in \texttt{arima.sim} are specified as:
\begin{equation} \label{arima.sim_doc}
X_t = \phi_1X_{t-1} + ??? + \phi_pX_{t-p} + \epsilon_t + \theta_1\epsilon_{t-1} + ??? + \theta_q \epsilon_{t-q}
\end{equation}
Therefore, we will have to reverse the sign of our parameter $\phi_1$ when using it in the model. Because the \texttt{knitr}package was used in R Sweave, the following results are unique for every new render of the report. Furthermore, since \eqref{process} is proportional to three degrees of randomness, we expect do expect some results to fall outside our expectations. Therefore, any comments apply on the general behaviour of the simulations, and specific aberrations are regarded as coincidental. To clearly distinguish between every of the ten numerical simulations, vivid colours will be indicating the simulation number throughout all coming graphs in this chapter and analytical results will be shown in black. 

<<eval=T,echo=F,fig.height=4>>=
N = 200
realisations = 10
ARMAmodels = matrix(NA, ncol = realisations, nrow = N)
for (i in 1:realisations){
  ARMAmodels[,i] = arima.sim(n=N, list(ar=0.9,ma=c(2,0.5)), innov = rnorm(200, mean = 0, sd = 0.2))
}

matplot(ARMAmodels, type = 'l', lty = 1, col = rainbow(realisations), xlab = 'Observation Number',
        ylab = 'Observation');
lines(1:N, matrix(0, ncol = 1, nrow = N))
title(main = 'ARMA(1,2) Simulations')
@

From the simulated models, we can clearly see the stochastic influence of the white-noise process. All models start at a different point, but are clearly stationary and have (after a sufficient number of observations) a mean of 0. Between the different realisations, we can see differing behaviour but the global trend is highly visible. 


\subsubsection{Covariance of the simulations}
The covariance was calculated numerically through \texttt{acf(type = "covariance")} and is shown below. 

<<eval=T,echo=F,fig.height=3.6>>=
COVs = matrix(NA, ncol = realisations, nrow = ml)

#Saving the ACF and PACF as lists
for (i in 1:realisations){
  COV = acf(ARMAmodels[,i], type = "covariance", plot = F)
  COVs[,i] = COV$acf
}

#Plot COVs
matplot(COVs, type = 'l', lty = 1, col = rainbow(realisations), xlab = 'Lag', ylab = 'ACF')
lines(1:ml, gs[1:ml], lwd = 4, col = 'black')
lines(1:ml, matrix(0, ncol = 1, nrow = ml))
legend(x = "topright", legend=c("Cov (num)", "Cov (analyt)"), col=c("red", "black"), lty= c(1,1), cex=0.5)
title(main = "ARMA(2,1) Covariance")
@

We can see that the numerical covariance shows a high deviation from the analytical result. We will regard this behaviour as coincidental, since the global decreasing trend is highly visible. In Table 3, the analytical and four numerical autocovariance functions from the simulations are shown.

\begin{table}[h] 
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
    k           & 0  & 1  & 2  & 3  & 4  & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline
    $\gamma(k)$   & \Sexpr{gs[1]} & \Sexpr{gs[2]} & \Sexpr{gs[3]} & \Sexpr{gs[4]} & \Sexpr{gs[5]} & \Sexpr{gs[6]}  & \Sexpr{gs[7]}     & \Sexpr{gs[8]}  & \Sexpr{gs[9]}  & \Sexpr{gs[10]}  &  \Sexpr{gs[11]}  \\ \hline
    COV[k,1]   & \Sexpr{(COVs[1,1])} & \Sexpr{(COVs[2,1])} & \Sexpr{(COVs[3,1])} & \Sexpr{(COVs[4,1])} & \Sexpr{(COVs[5,1])} & \Sexpr{(COVs[6,1])}  & \Sexpr{(COVs[7,1])}  & \Sexpr{(COVs[8,1])}  & \Sexpr{(COVs[9,1])}  & \Sexpr{(COVs[10,1])}  &  \Sexpr{(COVs[11,1])}  \\ \hline
    COV[k,2]   & \Sexpr{(COVs[1,2])} & \Sexpr{(COVs[2,2])} & \Sexpr{(COVs[3,2])} & \Sexpr{(COVs[4,2])} & \Sexpr{(COVs[5,2])} & \Sexpr{(COVs[6,2])}  & \Sexpr{(COVs[7,2])}  & \Sexpr{(COVs[8,2])}  & \Sexpr{(COVs[9,2])}  & \Sexpr{(COVs[10,2])}  &  \Sexpr{(COVs[11,2])}  \\ \hline
    COV[k,3]   & \Sexpr{(COVs[1,3])} & \Sexpr{(COVs[2,3])} & \Sexpr{(COVs[3,3])} & \Sexpr{(COVs[4,3])} & \Sexpr{(COVs[5,3])} & \Sexpr{(COVs[6,3])}  & \Sexpr{(COVs[7,3])}  & \Sexpr{(COVs[8,3])}  & \Sexpr{(COVs[9,3])}  & \Sexpr{(COVs[10,3])}  &  \Sexpr{(COVs[11,3])}  \\ \hline
    COV[k,4]   & \Sexpr{(COVs[1,4])} & \Sexpr{(COVs[2,4])} & \Sexpr{(COVs[3,4])} & \Sexpr{(COVs[4,4])} & \Sexpr{(COVs[5,4])} & \Sexpr{(COVs[6,4])}  & \Sexpr{(COVs[7,4])}  & \Sexpr{(COVs[8,4])}  & \Sexpr{(COVs[9,4])}  & \Sexpr{(COVs[10,4])}  &  \Sexpr{(COVs[11,4])}  \\ \hline
  \end{tabular}
  \caption{Autocovariance functions: analytical and numerical results}
\end{table}

\subsubsection{Autocorrelation of the simulations}
To show all ACF and PACF curves in the same graph, we export the results from the \texttt{acf}function with \texttt{ACF\$acf} to a rectangular matrix and plot them with \texttt{matplot}. Because a histogram style was deemed unreadable, the ACF was plotted here with lines. Any distinct peaks outside of the 95\% interval are regarded as coincidental. 

<<eval=T,echo=F,fig.height=3.6>>=
ACFs = matrix(NA, ncol = realisations, nrow = ml)
PACFs = matrix(NA, ncol = realisations, nrow = ml-1)

#Saving the ACF and PACF as lists
for (i in 1:realisations){
  ACF = acf(ARMAmodels[,i], type = "correlation", plot = F)
  ACFs[,i] = ACF$acf

  PACF = pacf(ARMAmodels[,i], plot = F)
  PACFs[,i] = PACF$acf
}

#Confidence interval
CI = qnorm((1 + 0.95)/2)/sqrt(N)

#Plot ACFs
matplot(ACFs, type = 'l', lty = 1, col = rainbow(realisations), xlab = 'Lag', ylab = 'ACF')
lines(1:ml, p[1:ml], lwd = 3, col = 'black')
lines(1:ml, matrix(0, ncol = 1, nrow = ml))
lines(1:ml, matrix(CI, ncol = 1, nrow = ml), col = 'blue', lty = 2)
lines(1:ml, matrix(-CI, ncol = 1, nrow = ml), col = 'blue', lty = 2)
legend(x = "topright", legend=c("ACF's (num)", "ACF (analyt)", "95% ci"), col=c("red", "black", 'blue'), lty= c(1,1,2), cex=0.5)
title(main = "ARMA(1,2) ACF's")
@

We can see that the numerical results correspond well to the analytical result. As is stated in [TSA](p 127), the ACF consists of damped exponential and harmonic functions from lag k = q + 1 - p = 2, and there is a clear linear behaviour before lag 2. All ACF's are expected to decay to zero within a meaningful time.  In Table 4, the analytical and four numerical autocorrelation functions from the simulations are shown.

\begin{table}[h] 
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
    k           & 0  & 1  & 2  & 3  & 4  & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline
    $\rho(k)$   & \Sexpr{p[1]} & \Sexpr{p[2]} & \Sexpr{p[3]} & \Sexpr{p[4]} & \Sexpr{p[5]} & \Sexpr{p[6]}  & \Sexpr{p[7]}     & \Sexpr{p[8]}  & \Sexpr{p[9]}  & \Sexpr{p[10]}  &  \Sexpr{p[11]}  \\ \hline
    ACF[k,1]   & \Sexpr{(ACFs[1,1])} & \Sexpr{(ACFs[2,1])} & \Sexpr{(ACFs[3,1])} & \Sexpr{(ACFs[4,1])} & \Sexpr{(ACFs[5,1])} & \Sexpr{(ACFs[6,1])}  & \Sexpr{(ACFs[7,1])}  & \Sexpr{(ACFs[8,1])}  & \Sexpr{(ACFs[9,1])}  & \Sexpr{(ACFs[10,1])}  &  \Sexpr{(ACFs[11,1])}  \\ \hline
    ACF[k,2]   & \Sexpr{(ACFs[1,2])} & \Sexpr{(ACFs[2,2])} & \Sexpr{(ACFs[3,2])} & \Sexpr{(ACFs[4,2])} & \Sexpr{(ACFs[5,2])} & \Sexpr{(ACFs[6,2])}  & \Sexpr{(ACFs[7,2])}  & \Sexpr{(ACFs[8,2])}  & \Sexpr{(ACFs[9,2])}  & \Sexpr{(ACFs[10,2])}  &  \Sexpr{(ACFs[11,2])}  \\ \hline
    ACF[k,3]   & \Sexpr{(ACFs[1,3])} & \Sexpr{(ACFs[2,3])} & \Sexpr{(ACFs[3,3])} & \Sexpr{(ACFs[4,3])} & \Sexpr{(ACFs[5,3])} & \Sexpr{(ACFs[6,3])}  & \Sexpr{(ACFs[7,3])}  & \Sexpr{(ACFs[8,3])}  & \Sexpr{(ACFs[9,3])}  & \Sexpr{(ACFs[10,3])}  &  \Sexpr{(ACFs[11,3])}  \\ \hline
    ACF[k,4]   & \Sexpr{(ACFs[1,4])} & \Sexpr{(ACFs[2,4])} & \Sexpr{(ACFs[3,4])} & \Sexpr{(ACFs[4,4])} & \Sexpr{(ACFs[5,4])} & \Sexpr{(ACFs[6,4])}  & \Sexpr{(ACFs[7,4])}  & \Sexpr{(ACFs[8,4])}  & \Sexpr{(ACFs[9,4])}  & \Sexpr{(ACFs[10,4])}  &  \Sexpr{(ACFs[11,4])}  \\ \hline
  \end{tabular}
  \caption{Autocorrelation functions: analytical and numerical results}
\end{table}


\subsubsection{Partial autocorrelation of the simulations}

<<eval=T,echo=F,fig.height=3.6>>=
#Plot PACFs
matplot(PACFs, type = 'h', lty = 1, col = rainbow(realisations), xlab = 'Lag', ylab = 'PACF')
lines(1:ml, diag(phi), lwd = 3, col = 'black')
lines(1:ml, matrix(0, ncol = 1, nrow = ml))
lines(1:ml, matrix(CI, ncol = 1, nrow = ml), col = 'blue', lty = 2)
lines(1:ml, matrix(-CI, ncol = 1, nrow = ml), col = 'blue', lty = 2)
legend(x = "topright", legend=c("PACF's (num)", "PACF (analyt)", "95% ci"), col=c("red", 'black', 'blue'), lty= c(1,1,2), cex=0.5)
title(main = "ARMA(1,2) PACF's")
@

We can see that the numerical results correspond even better to the analytical result. The PACF's show more consistency, though we can see the numerical noise when the analytical result has already decayed. In Table 5, the analytical and four numerical partial autocorrelation functions from the simulations are shown.

\begin{table}[h] 
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
  \hline
    k             & 1  & 2  & 3  & 4  & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline
    $\phi_{k,k}$   & \Sexpr{phi[1,1]} & \Sexpr{phi[2,2]} & \Sexpr{phi[3,3]} & \Sexpr{phi[4,4]} & \Sexpr{phi[5,5]} & \Sexpr{phi[6,6]}  & \Sexpr{phi[7,7]}     & \Sexpr{phi[8,8]}  & \Sexpr{phi[9,9]}  & \Sexpr{phi[10,10]}  \\ \hline
    PACF[k,1]   & \Sexpr{(PACFs[1,1])} & \Sexpr{(PACFs[2,1])} & \Sexpr{(PACFs[3,1])} & \Sexpr{(PACFs[4,1])} & \Sexpr{(PACFs[5,1])} & \Sexpr{(PACFs[6,1])}  & \Sexpr{(PACFs[7,1])}  & \Sexpr{(PACFs[8,1])}  & \Sexpr{(PACFs[9,1])}  & \Sexpr{(PACFs[10,1])}    \\ \hline
    PACF[k,2]   & \Sexpr{(PACFs[1,2])} & \Sexpr{(PACFs[2,2])} & \Sexpr{(PACFs[3,2])} & \Sexpr{(PACFs[4,2])} & \Sexpr{(PACFs[5,2])} & \Sexpr{(PACFs[6,2])}  & \Sexpr{(PACFs[7,2])}  & \Sexpr{(PACFs[8,2])}  & \Sexpr{(PACFs[9,2])}  & \Sexpr{(PACFs[10,2])}    \\ \hline
    PACF[k,3]   & \Sexpr{(PACFs[1,3])} & \Sexpr{(PACFs[2,3])} & \Sexpr{(PACFs[3,3])} & \Sexpr{(PACFs[4,3])} & \Sexpr{(PACFs[5,3])} & \Sexpr{(PACFs[6,3])}  & \Sexpr{(PACFs[7,3])}  & \Sexpr{(PACFs[8,3])}  & \Sexpr{(PACFs[9,3])}  & \Sexpr{(PACFs[10,3])}  \\ \hline
    PACF[k,4]   & \Sexpr{(PACFs[1,4])} & \Sexpr{(PACFs[2,4])} & \Sexpr{(PACFs[3,4])} & \Sexpr{(PACFs[4,4])} & \Sexpr{(PACFs[5,4])} & \Sexpr{(PACFs[6,4])}  & \Sexpr{(PACFs[7,4])}  & \Sexpr{(PACFs[8,4])}  & \Sexpr{(PACFs[9,4])}  & \Sexpr{(PACFs[10,4])}  \\ \hline
  \end{tabular}
  \caption{Partial autocorrelation functions: analytical and numerical results}
\end{table}

\subsubsection{Discussion}
Even though our 10 simulations differ from each other, through the dependance on three degrees of randomness, their respective behaviour can be documented well through the ACF's and PACF's. The simulations follow the theoretical results well enough to say that they are equivalent.


%------------------------- Question 2.2 -------------------------%
\newpage
\section{Question 2.2} \label{Q2.2}
%----------- the model -----------%
\subsection{The multiplicative seasonal process}
To predict the energy consumption of the city of Copenhagen, we have been given the folowing process:
\begin{equation} \label{energyprocess}
(1 - 0.5B + 0.5B^2)  (1 - 0.95B^{12})  (Y_t - \mu) = \epsilon_t 
\end{equation}
where $\epsilon_t$ is a white-noise process with $V[\epsilon_t]$ = $\sigma^2_{\epsilon}$ = $7^2$ and $\mu$ = 204. In \eqref{energyprocess} we recognise an annual dependance: $B^{12}Y_t = Y_{t-12}$. Surely, the energy consumption during wintertime must be higher than during the summer because of additional lighting and heating, it makes sense to introduce a seasonal component (12 months) to our model. For ease of notation, we will write \eqref{energyprocess} as:
\begin{equation} \label{energyprocess_better}
\phi(B) \Phi(B^{12}) (Y_t - \mu) = \epsilon_t
\end{equation}
with $\phi_0 = 1, \phi_1 = -0.5, \phi_2 = 0.5, \Phi_0 = 1$ and $\Phi_1 = 0.95$. Formally, according to [TSA](p 132, eq 5.127), we identify \eqref{energyprocess} as a \textit{multiplicative} $(2, 0, 0)\times (1, 0, 0)_{12}$ \textit{seasonal model} and since d = D = 0 the model is stationary.

%----------- Predictions -----------%
\subsection{Making a prediction}
Given the observations of the past fifteen months, we want to make a prediction for the two next months. For further ease of notation, we will subtract the mean $\mu$ from all observations, so that we effectively obtain a process with a mean value of zero: $Z_t$ = $Y_t$ - $\mu$. Hence, $Z_t$ = 21 and $Z_{t-15}$ = -14. The result are plotted below.

<<eval=T,echo=F,fig.height=3.5>>=
sige = 7
mu = 204
months = c(-15:0)
cons = c(190, 208, 213, 223, 237, 214, 221, 201, 191, 184, 184, 189, 188, 207, 221, 225) - mu
t = length(cons)

plot(months, cons, type = 'l', ylim = c(-25,35), xlab = 'Month number k', ylab = 'E. consumption [GWh]', col = 'red')
points(months, cons, col = 'red')
lines(months, matrix(0, ncol = length(months)), col = 8, lty = 2)
legend(x = "topleft", legend=c("Observations", "Mean (est.)"), col=c("red", 80), lty=1:2, cex=0.5)
title(main = 'Electricity consumption in Copenhagen')

pred1 = 0.5*cons[t] - 0.5*cons[t-1] + 0.95*cons[t-11] - 0.475*cons[t-12] + 0.475*cons[t-13] 
pred2 = 0.5*pred1   - 0.5*cons[t] + 0.95*cons[t-10] - 0.475*cons[t-11] + 0.475*cons[t-12] 

varZ1 = sige^2 + 0.5*sige^2
varZ2 = 1.25 * varZ1

ciZ1 = c(pred1 - qnorm(0.975)*sqrt(varZ1), pred1 + qnorm(0.975)*sqrt(varZ1))
ciZ2 = c(pred2 - qnorm(0.975)*sqrt(varZ2), pred2 + qnorm(0.975)*sqrt(varZ2))
@
Adapting \eqref{energyprocess} to the new series $\{ Z_t \}$, we can see that 
\begin{equation} \label{Zprocess}
Z_t - 0.5 Z_{t-1} + 0.5 Z_{t-2} - 0.95 Z_{t-12} + 0.475 Z_{t-13} - 0.475 Z_{t-14} = \epsilon_t 
\end{equation}
so that the current observation is dependant on the past two months of this year cycle and the previous one. In other words, the temperature in January of 2017 will depend on that of December and November in 2016 and 2015. To formalize this behaviour, we will follow the procedure in [TSA](p138, eq 5.153) and beyond. We can write \eqref{Zprocess} as:
\begin{equation} 
Z_t + \varphi_1 Z_{t-1} + \varphi_2 Z_{t-2} + \varphi_{12} Z_{t-12} + \varphi_{13} Z_{t-13} + \varphi_{14} Z_{t-14} = \epsilon_t 
\end{equation}
where $\varphi_i$ correspond to their respective values in \eqref{Zprocess} and $\varphi_3$ through $\varphi_{11}$ are zero. The forecast equation becomes:
\begin{align*} \label{Zforecast}
\hat{Z}_{t+k|t} = &- \varphi_1 E[Z_{t+k-1}|Z_t, Z_{t-1}, ...] - \varphi_2 E[Z_{t+k-2}|Z_t, Z_{t-1}, ...] \\
&- \varphi_{12} E[Z_{t+k-12}|Z_t, Z_{t-1}, ...] - \varphi_{13} E[Z_{t+k-13}|Z_t, Z_{t-1}, ...] \\
&- \varphi_{14} E[Z_{t+k-14}|Z_t, Z_{t-1}, ...] + E[\epsilon_{t+k}|Z_t, Z_{t-1}, ...]
\end{align*}
where every E[$\epsilon_{t+k}|Z_t, Z_{t-1}, ...$] = 0. For the two predictions, this is effectively:
\begin{align*} 
\hat{Z}_{t+1|t} &= - \varphi_1 E[Z_{t}|...] - \varphi_2 E[Z_{t-1}|...] - \varphi_{12} E[Z_{t-11}|...] - \varphi_{13} E[Z_{t-12}|...] - \varphi_{14} E[Z_{t-13}|...] \\
&= 0.5 Z_{t} - 0.5 Z_{t-1} + 0.95 Z_{t-11} - 0.475 Z_{t-12} + 0.475 Z_{t-13} = \Sexpr{pred1}
\end{align*}
\begin{align*} 
\hat{Z}_{t+2|t} &= - \varphi_1 E[\hat{Z}_{t+1}|...] - \varphi_2 E[Z_{t}|...] - \varphi_{12} E[Z_{t-10}|...] - \varphi_{13} E[Z_{t-11}|...] - \varphi_{14} E[Z_{t-12}|...]  \\
&= 0.5 \hat{Z}_{t+1|t} - 0.5 Z_{t} + 0.95 Z_{t-10} - 0.475 Z_{t-11} + 0.475 Z_{t-12} = \Sexpr{pred2}
\end{align*}
For the \{$Y_t$\} process, the predictions are therefore $\hat{Y}_{t+1|t}$ = \Sexpr{pred1 + mu} GWh and $\hat{Y}_{t+2|t}$ = \Sexpr{pred2 + mu} GWh. The observations are said to be observed without variance, so for the variance of the two predictions, we will use use \eqref{Zprocess} to write:

\begin{align}
V[Z_{t+1}] &= E[\epsilon^2_t] + E[0.5Z^2_t] + ... + E[0.475 Z^2_{t-13}] + E[0.5 \: \epsilon_t\:Z_t] \\
&= \sigma^2_{\epsilon} + 0.5 \gamma_{\epsilon X}(0) = 1.5 \sigma^2_{\epsilon} = \Sexpr{varZ1}
\end{align}
where all $E[Z^2_{t-k}]$ = 0 (observations without variance), $E[\epsilon_t \: Z_{t-k}]$ = 0 (present errors are independant of previous observations) and $\gamma_{\epsilon X}(0)$ = $\theta_0 \sigma^2_{\epsilon}$ according to [TSA](p 126, eq 5.97). Then, we also have:

\begin{align}
V[Z_{t+2}] &= E[\epsilon^2_{t+1}] + E[0.25Z^2_{t+1}] + ... + E[0.475 Z^2_{t-12}] + E[0.5 \: \epsilon_{t+1}\:Z_{t+1}] \\
&= \sigma^2_{\epsilon} + 0.25 V[Z_{t+1}] + 0.5 \gamma_{\epsilon X}(0) = 1.5 \sigma^2_{\epsilon} + 0.25 V[Z_{t+1}] \\
&= 1.25 V[Z_{t+1}] = \Sexpr{varZ2}
\end{align}
with the same remarks as before. To finally obtain the confidence interval, we will use [TSA](p 137, eq 5.151) with \texttt{qnorm(0.975)} to account for the 2.5\% on both sides of the distribution. For $\hat{Z}_{t+1|t}$ we obtain the confidence interval [\Sexpr{ciZ1}], for $\hat{Z}_{t+2|t}$ we obtain the confidence interval [\Sexpr{ciZ2}]. For the \{$Y_t$\} process, we just have to add the mean value we substracted before. The results are plotted below, where we can see the seasonal decrease to set in, with the 95\%-interval in blue:

<<eval=T,echo=F,fig.height=3.5>>=
months = c(-15:2)
ci = matrix(c(225, 225, ciZ1 + mu, ciZ2 + mu), nrow = 3, ncol = 2, byrow = T)
cons = c((cons + mu), (pred1 + mu), (pred2 + mu))
t = length(cons)

plot(months, cons, type = 'l', ylim = c(180, 250), xlab = 'Month number k', ylab = 'E. consumption [GWh]', col = 'red')
points(months, cons, col = 'red')
points(months[17:18], cons[17:18])
lines(months[16:18], cons[16:18])
polygon(c(0:2, rev(0:2)), c(ci[,1], rev(ci[,2])),
        col=rgb(0.02,0.02,0.9,alpha=0.1) , border = NA)
lines(months, matrix(mu, ncol = length(months)), col = 8, lty = 2)
legend(x = "topleft", legend=c("Observations", "Mean (est.)", "Predictions", "95% interval"), col=c("red", 80, "black", rgb(0.02,0.02,0.9,alpha=0.1)), lty=c(1,2,1,1), cex=0.5)
title(main = 'Electricity consumption in Copenhagen')
@

We predicted:\\

$\hat{Y}_{t+1|t}$ = \Sexpr{pred1 + mu} GWh within the 95\% interval [\Sexpr{ciZ1 + mu}] \\

$\hat{Y}_{t+2|t}$ = \Sexpr{pred2 + mu} GWh within the 95\% interval [\Sexpr{ciZ2 + mu}]


%------------------------- Question 2.3 -------------------------%
\newpage
\section{Question 2.3}
<<eval=T,echo=F,fig.height=3>>=
total_observ = 250
makeModel = function(ARcf, MAcf, N){
  ARiMAmodel = arima.sim(n=N, list(ar=-ARcf,ma=MAcf), innov = rnorm(N, mean = 0, sd = 1))
  
  #Plot of the model
  plot(1:N, ARiMAmodel, type = 'l', xlab = 'Observation Number', 
       ylab = 'Observation', col = 'red')
  grid(total_observ/12, total_observ/24)
  lines(1:N, matrix(0, ncol = N), lwd = 0.3)
  legend(x = "topleft", legend=c("Observations", "Grid", "x-axis"), xpd = TRUE, inset=c(0,-0.4), 
         col=c("red", 8, "black"), lty=c(1,2,1), cex=0.5)
  title(main = 'Model Simulation')
  
  #ACF & PACF
  par(mfrow=c(1,2))
  acf(ARiMAmodel, lag.max = 25, main = 'ACF') 
  legend(x = "topleft", legend=c("ACF", "95% ci"), col=c("black", 'blue'), xpd = TRUE, inset=c(0,-0.5), lty= c(1,2), cex=0.5)
  pacf(ARiMAmodel, lag.max = 25, main = 'PACF')
  legend(x = "topleft", legend=c("PACF", "95% ci"), col=c("black", 'blue'), xpd = TRUE, inset=c(0,-0.5), lty= c(1,2), cex=0.5)

  return(ARiMAmodel) 
}
@
Here, we will simalute various seasonal processes. Since the ARIMA-simulator in R does not have a seasonal module, we will have to formulate the models as standard ARIMA-processes. Furthermore, the given coefficients are not specified as in \eqref{arima.sim_doc}, so we will again reverse the sign of the AR parameters $\phi_i$ and $\Phi_j$ when using them in the model. To be able to see any seasonal parameters in the ACF and PACF, their max lag was set to 25. To see how the model behaves, we choose this lag to be around 10\% of the amount of observations, so we construct 250 observations. The models run through at least 20 season cycles. Similar comments regarding general behaviour apply, since every compile makes for unique graphs.

%----------- 1 -----------%
\subsection{$(1, 0, 0)\times (0, 0, 0)_{12}$}
\begin{equation}
(1 + \phi_1 B)Y_t = \epsilon_t
\end{equation}
where $\phi_1$ = 0.85. This is a standard AR(1)-process.

<<eval=T,echo=F,fig.height=3>>=
process = makeModel(0.85, 0, total_observ); E = mean(process)
@
The first model appears random but structured, with a high frequency. We can also see this in the ACF: a high-frequency damped sine function without any seasonal repitition, decreasing to zero within a meaningful timeframe. The PACF shows us one spike, accounting for the 1st order of the AR(1) model. The model is stationary and not seasonal.

\newpage
%----------- 2 -----------%
\subsection{$(0, 0, 0)\times (1, 0, 0)_{12}$}
\begin{equation}
(1 + \Phi_{12} B^{12})Y_t = \epsilon_t
\end{equation}
where $\Phi_{12}$ = - 0.8. This is an AR(12)-process with $\Phi_1$ = ... = $\Phi_{11}$ =  0.

<<eval=T,echo=F,fig.height=3>>=
process = makeModel(c(0,0,0,0,0,0,0,0,0,0,0,-0.8), 0, total_observ)
@
The second model appears quite random and we see more fluctuation from the x-axis than before. In the ACF we can see a decreasing seasonal repitition of lag 0, 12, 24, etc, with occasionally spikes in between which are considered as noise. Because all AR-parameters but $\Phi_{12}$ are 0, this graph looks like a standard AR(1) was stretched out twelvefold. The PACF shows us one spike, accounting for the 12th order of the AR(12) model. The model is stationary (decreasing ACF, z = $\sqrt[12]{0.8} < 1$ ) and seasonal.

\newpage
%----------- 3 -----------%
\subsection{$(1, 0, 0)\times (0, 0, 1)_{12}$}
\begin{equation}
(1 + \phi_{1} B)Y_t = (1 + \Theta_{12} B^{12}) \epsilon_t
\end{equation}
where $\phi_{1}$ = - 0.8 and $\Theta_{12}$ = 0.7. This is an ARMA(1, 12)-process with $\Theta_1$ = ... = $\Theta_{11}$ =  0.

<<eval=T,echo=F,fig.height=3>>=
process = makeModel(-0.8, c(0,0,0,0,0,0,0,0,0,0,0,0.7), total_observ)
@
The third model appears random but less dense than the previous models. In the ACF, we can spot a second peak at lag 12, and a third one at 24, accounting for the 12th order of the MA(12) part of the model. The PACF shows us one spike, accounting for the 1st order of the AR(1) part of the model, though another peak appears at lag 13. We would expect this to be zero after lag k = q + 1 - p = 12. The model is stationary and seasonal.


\newpage
%----------- 4 -----------%
\subsection{$(1, 0, 0)\times (1, 0, 0)_{12}$}
\begin{align}
(1 + \phi_{1} B)(1 + \Phi_{12} B^{12})Y_t &= \epsilon_t \\
(1 + \phi_{1} B + \Phi_{12} B^{12} + \phi_1\Phi_{12} B^{13})Y_t &= \epsilon_t \\
(1 + \varphi_{1} B + \varphi_{12} B^{12} + \varphi_{13} B^{13})Y_t &= \epsilon_t
\end{align}
where $\phi_{1}$ = - 0.7 and $\Phi_{12}$ = 0.8. This is an AR(13)-process with:
\[ \varphi_1 = - 0.7, \varphi_2 = ... = \varphi_{11} =  0, \varphi_{12} = 0.8, \varphi_{13} = - 0.56\]

<<eval=T,echo=F,fig.height=3>>=
process = makeModel(c(-0.7,0,0,0,0,0,0,0,0,0,0,0.8,-0.56), 0, total_observ)
@
The fourth model appears random and less dense again, with what looks like a more seasonal structure. In the ACF, we can spot a damped sine function with peaks at lag 0, 12, 24, etc, accounting for the seasonal dependancy of 12. The PACF is more complicated: we can see clear peaks at lag 0, 12 and 13. Both the seasonal dependancy (12) and the order of the model (13) are clearly visible, all others are zero after lag 13 as expected. The model is stationary and seasonal.


\newpage
%----------- 5 -----------%
\subsection{$(2, 0, 0)\times (1, 0, 0)_{12}$}
\begin{align}
(1 + \phi_{1} B + \phi_{2} B^2)(1 + \Phi_{12} B^{12})Y_t &=  \epsilon_t \\
(1 + \phi_{1} B + \phi_{2} B^2 + \Phi_{12} B^{12} + \phi_{1} \Phi_{12} B^{13} + \phi_{2} \Phi_{12} B^{14})Y_t &=  \epsilon_t \\
(1 + \varphi_{1} B + \varphi_{2} B^2 +\varphi_{12} B^{12} + \varphi_{13} B^{13} + \varphi_{14} B^{14})Y_t &= \epsilon_t
\end{align}
where $\phi_{1}$  = $\phi_{2}$ = 0.6 and $\Phi_{12}$ = 0.8. This is the same model used in Section \ref{Q2.2}, an AR(14)-process, with:
\[\varphi_{1} = \varphi_{2} = 0.6, \varphi_3 = ... = \varphi_{11} =  0, \varphi_{12} = 0.8, \varphi_{13} = \varphi_{14} = 0.48\]

<<eval=T,echo=F,fig.height=3>>=
process = makeModel(c(0.6,0.6,0,0,0,0,0,0,0,0,0,0.8,0.48,0.48), 0, total_observ)
@

The fifth model appears dense again, with a high frequency. In the ACF, we can spot damped, individual peaks at lag 0, 12, 24, etc, accounting for the seasonal dependancy of 12. The PACF is more complicated: we can see clear peaks at lag 1, 2, 12 and 14. Both the seasonal dependancy (12) and the order of the model (14) are clearly visible, all others are zero after lag 14 as expected. The model is stationary and seasonal.

\newpage
%----------- 6 -----------%
\subsection{$(0, 0, 1)\times (0, 0, 1)_{12}$}
\begin{align}
Y_t &=  (1 + \theta_{1} B)(1 + \Theta_{12} B^{12})\epsilon_t \\
&=  (1 + \theta_{1} B + \Theta_{12} B^{12} + \theta_{1}\Theta_{12} B^{13})\epsilon_t \\
&=  (1 + \vartheta_{1} B + \vartheta_{12} B^{12} + \vartheta_{13} B^{13})\epsilon_t \\
\end{align}
where $\theta_{1}$ = -0.7 and $\Theta_{12}$ = 0.8. This is an MA(13)-process with:
\[\vartheta_{1} = -0.7, \vartheta_2 = ... = \vartheta_{11} =  0, \vartheta_{12} = 0.8, \vartheta_{13} = -0.56\]

<<eval=T,echo=F,warning=F,fig.height=3>>=
process = makeModel(0, c(-0.7,0,0,0,0,0,0,0,0,0,0,0.8,-0.56), total_observ)
@
The last model appears rather similar to the model from before, though this could be coincidence. In the ACF, we can spot distinct peaks at lag 0, 1 and 12, accounting for the seasonal dependancy of 12. The PACF is even more complicated: we can see clear peaks at lag 1, 11, 12, and 13 without a clear underlying structure, though the seasonal dependancy (12) and the order of the model (13) are clearly visible. The model is stationary and seasonal.

\subsection{Discussion}
What we have learned from the previous models is that processes without differencing (d = D = 0) [TSA](p 132, eq 5.127) and with parameters smaller than 1 are stationary. Any seasonal parameter \textit{s} will give the process a seasonal dependancy, as can be seen in the (P)ACF plots, and a season of length 12$\cdot$n is also visible in a season of length 12 (n = 1, 2, ...). \textit{s} can therefore be determined from the peaks in the ACF. The model order can be estimated from the peaks in the PACF, where the results will show an exponentially or harmonically damped form from lag k = q + 1 - p. 

\newpage
\section{Appendix}
\subsection{Code: setup}
<<eval=F,echo=T>>=
library(knitr)
rm(list=ls())
cat("\014")
options(scipen = 1, digits = 3)
@
\subsection{Code: second order moment calculations}
<<eval=F,echo=T>>=
#Solve linear system for autocovariance function  --->  get ACF
ml = 24    #maximum lag in all coming plots
coeff = matrix(c(1,-.9,0,0, -.9,1,0,0, 0,-.9,1,0, 0,0,-.9,1), 4, 4, byrow = T)
b = c(0.3342, 0.138, 0.02, 0)
gs = solve(coeff, b)
p = gs/gs[1]

for (i in 4:ml){
  gs = c(gs, 0.9*gs[i]) 
  p[i] = gs[i]/gs[1]
}

#Get PACF
phi = matrix(0, nrow = ml, ncol = ml)
phi[1,1] = p[2]

for (k in 1:(ml-1)){
  numerator = 0
  denominator = 0
  
  for (j in 1:k){
    numerator = numerator + phi[k,j]*p[k+2-j]
    denominator = denominator + phi[k,j]*p[j+1]
  }
  
  phi[k+1, k+1] = (p[k+2] - numerator)/(1 - denominator)

  for (j in 1:k){ 
    phi[k+1, j] = phi[k, j] - phi[k+1, k+1]*phi[k, k+1-j] 
    }
}
@
\subsection{Code: model simulation}
<<eval=F,echo=T,fig.height=4>>=
#Making the model: 
N = 200
realisations = 10
ARMAmodels = matrix(NA, ncol = realisations, nrow = N)
for (i in 1:realisations){
  ARMAmodels[,i] = arima.sim(n=N, list(ar=0.9,ma=c(2,0.5)), 
                  innov = rnorm(200, mean = 0, sd = 0.2))
}

matplot(ARMAmodels, type = 'l', lty = 1, col = rainbow(realisations), 
        xlab = 'Observation Number', ylab = 'Observation');
lines(1:N, matrix(0, ncol = 1, nrow = N))
title(main = 'ARMA(1,2) Simulations')

#Numerically extract COV, ACF & PACF: start with allocationg space
COVs = matrix(NA, ncol = realisations, nrow = ml)
ACFs = matrix(NA, ncol = realisations, nrow = ml)
PACFs = matrix(NA, ncol = realisations, nrow = ml-1)

#Saving the COV, ACF & PACF as lists
for (i in 1:realisations){
  COV = acf(ARMAmodels[,i], type = "covariance", plot = F)
  COVs[,i] = COV$acf
  
  ACF = acf(ARMAmodels[,i], type = "correlation", plot = F)
  ACFs[,i] = ACF$acf

  PACF = pacf(ARMAmodels[,i], plot = F)
  PACFs[,i] = PACF$acf
}

#Plot COVs
matplot(COVs, type = 'l', lty = 1, col = rainbow(realisations),
        xlab = 'Lag', ylab = 'ACF')
lines(1:ml, gs[1:ml], lwd = 4, col = 'black')
lines(1:ml, matrix(0, ncol = 1, nrow = ml))
legend(x = "topright", legend=c("Cov (num)", "Cov (analyt)"), 
       col=c("red", "black"), lty= c(1,1), cex=0.5)
title(main = "ARMA(2,1) Covariance")

#Confidence interval
CI = qnorm((1 + 0.95)/2)/sqrt(N)

#Plot ACFs
matplot(ACFs, type = 'l', lty = 1, col = rainbow(realisations), 
        xlab = 'Lag', ylab = 'ACF')
lines(1:ml, p[1:ml], lwd = 3, col = 'black')
lines(1:ml, matrix(0, ncol = 1, nrow = ml))
lines(1:ml, matrix(CI, ncol = 1, nrow = ml), col = 'blue', lty = 2)
lines(1:ml, matrix(-CI, ncol = 1, nrow = ml), col = 'blue', lty = 2)
legend(x = "topright", legend=c("ACF's (num)", "ACF (analyt)", "95% ci"), 
       col=c("red", "black", 'blue'), lty= c(1,1,2), cex=0.5)
title(main = "ARMA(1,2) ACF's")

#Plot PACFs
matplot(PACFs, type = 'h', lty = 1, col = rainbow(realisations), 
        xlab = 'Lag', ylab = 'PACF')
lines(1:ml, diag(phi), lwd = 3, col = 'black')
lines(1:ml, matrix(0, ncol = 1, nrow = ml))
lines(1:ml, matrix(CI, ncol = 1, nrow = ml), col = 'blue', lty = 2)
lines(1:ml, matrix(-CI, ncol = 1, nrow = ml), col = 'blue', lty = 2)
legend(x = "topright", legend=c("PACF's (num)", "PACF (analyt)", "95% ci"), 
       col=c("red", 'black', 'blue'), lty= c(1,1,2), cex=0.5)
title(main = "ARMA(1,2) PACF's")
@
\subsection{Code: electricity consumption}
<<eval=F,echo=T,fig.height=3.5>>=
#Data:
sige = 7
mu = 204
months = c(-15:0)
cons = c(190, 208, 213, 223, 237, 214, 221, 201, 191, 184, 184, 189, 188, 207, 
         221, 225) - mu
t = length(cons)

plot(months, cons, type = 'l', ylim = c(-25,35), xlab = 'Month number k',
     ylab = 'E. consumption [GWh]', col = 'red')
points(months, cons, col = 'red')
lines(months, matrix(0, ncol = length(months)), col = 8, lty = 2)
legend(x = "topleft", legend=c("Observations", "Mean (est.)"), col=c("red", 80), 
       lty=1:2, cex=0.5)
title(main = 'Electricity consumption in Copenhagen')

#Make predictions:
pred1 = 0.5*cons[t] - 0.5*cons[t-1] + 0.95*cons[t-11] - 0.475*cons[t-12] + 0.475*cons[t-13] 
pred2 = 0.5*pred1   - 0.5*cons[t] + 0.95*cons[t-10] - 0.475*cons[t-11] + 0.475*cons[t-12] 

varZ1 = sige^2 + 0.5*sige^2
varZ2 = 1.25 * varZ1

ciZ1 = c(pred1 - qnorm(0.975)*sqrt(varZ1), pred1 + qnorm(0.975)*sqrt(varZ1))
ciZ2 = c(pred2 - qnorm(0.975)*sqrt(varZ2), pred2 + qnorm(0.975)*sqrt(varZ2))

#Remake plot
months = c(-15:2)
ci = matrix(c(225, 225, ciZ1 + mu, ciZ2 + mu), nrow = 3, ncol = 2, byrow = T)
cons = c((cons + mu), (pred1 + mu), (pred2 + mu))
t = length(cons)

plot(months, cons, type = 'l', ylim = c(180, 250), xlab = 'Month number k', 
     ylab = 'E. consumption [GWh]', col = 'red')
points(months, cons, col = 'red')
points(months[17:18], cons[17:18])
lines(months[16:18], cons[16:18])
polygon(c(0:2, rev(0:2)), c(ci[,1], rev(ci[,2])),
        col=rgb(0.02,0.02,0.9,alpha=0.1) , border = NA)
lines(months, matrix(mu, ncol = length(months)), col = 8, lty = 2)
legend(x = "topleft", legend=c("Observations", "Mean (est.)", "Predictions", 
      "95% interval"), col=c("red", 80, "black", rgb(0.02,0.02,0.9,alpha=0.1)), 
      lty=c(1,2,1,1), cex=0.5)
title(main = 'Electricity consumption in Copenhagen')
@
\subsection{Code: simulating seasonal processes}
<<eval=F,echo=T,fig.height=3>>=
#Make a function for all coming simulations
total_observ = 250
makeModel = function(ARcf, MAcf, N){
  ARiMAmodel = arima.sim(n=N, list(ar=-ARcf,ma=MAcf), 
                         innov = rnorm(N, mean = 0, sd = 1))
  
  #Plot of the model
  plot(1:N, ARiMAmodel, type = 'l', xlab = 'Observation Number', 
       ylab = 'Observation', col = 'red')
  grid(total_observ/12, total_observ/24)
  lines(1:N, matrix(0, ncol = N), lwd = 0.3)
  legend(x = "topleft", legend=c("Observations", "Grid", "x-axis"), xpd = TRUE, 
         inset=c(0,-0.4), col=c("red", 8, "black"), lty=c(1,2,1), cex=0.5)
  title(main = 'Model Simulation')
  
  #ACF & PACF
  par(mfrow=c(1,2))
  acf(ARiMAmodel, lag.max = 25, main = 'ACF') 
  legend(x = "topleft", legend=c("ACF", "95% ci"), col=c("black", 'blue'), xpd = TRUE, 
         inset=c(0,-0.5), lty= c(1,2), cex=0.5)
  pacf(ARiMAmodel, lag.max = 25, main = 'PACF')
  legend(x = "topleft", legend=c("PACF", "95% ci"), col=c("black", 'blue'), xpd = TRUE, 
         inset=c(0,-0.5), lty= c(1,2), cex=0.5)

  return(ARiMAmodel) 
}
@
<<eval=F,echo=T,fig.height=3>>=
process = makeModel(0.85, 0, total_observ)
process = makeModel(c(0,0,0,0,0,0,0,0,0,0,0,-0.8), 0, total_observ)
process = makeModel(-0.8, c(0,0,0,0,0,0,0,0,0,0,0,0.7), total_observ)
process = makeModel(c(-0.7,0,0,0,0,0,0,0,0,0,0,0.8,-0.56), 0, total_observ)
process = makeModel(c(0.6,0.6,0,0,0,0,0,0,0,0,0,0.8,0.48,0.48), 0, total_observ)
process = makeModel(0, c(-0.7,0,0,0,0,0,0,0,0,0,0,0.8,-0.56), total_observ)
@

\end{document}

